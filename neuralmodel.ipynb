{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('TASK-ML-INTERN.csv')\n",
    "\n",
    "# Drop unnecessary column\n",
    "df.drop('hsi_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, threshold=4):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    return df[~((df<lower_bound)| (df>upper_bound)).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (500, 449)\n",
      "New shape after removing outliers: (468, 450)\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = remove_outliers_iqr(df)\n",
    "df_cleaned.reset_index(inplace = True)\n",
    "print(\"Original shape:\", df.shape)\n",
    "print(\"New shape after removing outliers:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>vomitoxin_ppb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.416181</td>\n",
       "      <td>0.396844</td>\n",
       "      <td>0.408985</td>\n",
       "      <td>0.372865</td>\n",
       "      <td>0.385293</td>\n",
       "      <td>0.365390</td>\n",
       "      <td>0.355226</td>\n",
       "      <td>0.343350</td>\n",
       "      <td>0.344837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710280</td>\n",
       "      <td>0.717482</td>\n",
       "      <td>0.715078</td>\n",
       "      <td>0.705379</td>\n",
       "      <td>0.696691</td>\n",
       "      <td>0.692793</td>\n",
       "      <td>0.711369</td>\n",
       "      <td>0.697679</td>\n",
       "      <td>0.704520</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.415797</td>\n",
       "      <td>0.402956</td>\n",
       "      <td>0.402564</td>\n",
       "      <td>0.396014</td>\n",
       "      <td>0.397192</td>\n",
       "      <td>0.389634</td>\n",
       "      <td>0.375671</td>\n",
       "      <td>0.363689</td>\n",
       "      <td>0.373883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684011</td>\n",
       "      <td>0.697271</td>\n",
       "      <td>0.701995</td>\n",
       "      <td>0.696077</td>\n",
       "      <td>0.701012</td>\n",
       "      <td>0.677418</td>\n",
       "      <td>0.696921</td>\n",
       "      <td>0.696544</td>\n",
       "      <td>0.689054</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.389023</td>\n",
       "      <td>0.371206</td>\n",
       "      <td>0.373098</td>\n",
       "      <td>0.373872</td>\n",
       "      <td>0.361056</td>\n",
       "      <td>0.349709</td>\n",
       "      <td>0.333882</td>\n",
       "      <td>0.330841</td>\n",
       "      <td>0.328925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683054</td>\n",
       "      <td>0.669286</td>\n",
       "      <td>0.663179</td>\n",
       "      <td>0.676165</td>\n",
       "      <td>0.676591</td>\n",
       "      <td>0.655951</td>\n",
       "      <td>0.658945</td>\n",
       "      <td>0.670989</td>\n",
       "      <td>0.665176</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.468837</td>\n",
       "      <td>0.473255</td>\n",
       "      <td>0.462949</td>\n",
       "      <td>0.459335</td>\n",
       "      <td>0.461672</td>\n",
       "      <td>0.459824</td>\n",
       "      <td>0.458194</td>\n",
       "      <td>0.427737</td>\n",
       "      <td>0.415360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742782</td>\n",
       "      <td>0.730801</td>\n",
       "      <td>0.736787</td>\n",
       "      <td>0.730044</td>\n",
       "      <td>0.751437</td>\n",
       "      <td>0.738497</td>\n",
       "      <td>0.742446</td>\n",
       "      <td>0.754657</td>\n",
       "      <td>0.733474</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.483352</td>\n",
       "      <td>0.487274</td>\n",
       "      <td>0.469153</td>\n",
       "      <td>0.487648</td>\n",
       "      <td>0.464026</td>\n",
       "      <td>0.451152</td>\n",
       "      <td>0.458229</td>\n",
       "      <td>0.440782</td>\n",
       "      <td>0.426193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770227</td>\n",
       "      <td>0.773013</td>\n",
       "      <td>0.761431</td>\n",
       "      <td>0.763488</td>\n",
       "      <td>0.762473</td>\n",
       "      <td>0.744012</td>\n",
       "      <td>0.775486</td>\n",
       "      <td>0.760431</td>\n",
       "      <td>0.751988</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>495</td>\n",
       "      <td>0.478140</td>\n",
       "      <td>0.444033</td>\n",
       "      <td>0.442120</td>\n",
       "      <td>0.437473</td>\n",
       "      <td>0.428672</td>\n",
       "      <td>0.413238</td>\n",
       "      <td>0.417758</td>\n",
       "      <td>0.420388</td>\n",
       "      <td>0.413290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747858</td>\n",
       "      <td>0.730535</td>\n",
       "      <td>0.716969</td>\n",
       "      <td>0.739297</td>\n",
       "      <td>0.724827</td>\n",
       "      <td>0.720484</td>\n",
       "      <td>0.740626</td>\n",
       "      <td>0.740116</td>\n",
       "      <td>0.721839</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>496</td>\n",
       "      <td>0.409367</td>\n",
       "      <td>0.394941</td>\n",
       "      <td>0.380236</td>\n",
       "      <td>0.375340</td>\n",
       "      <td>0.346122</td>\n",
       "      <td>0.354650</td>\n",
       "      <td>0.361170</td>\n",
       "      <td>0.342974</td>\n",
       "      <td>0.352137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670232</td>\n",
       "      <td>0.659045</td>\n",
       "      <td>0.661587</td>\n",
       "      <td>0.658422</td>\n",
       "      <td>0.644254</td>\n",
       "      <td>0.646479</td>\n",
       "      <td>0.656779</td>\n",
       "      <td>0.646700</td>\n",
       "      <td>0.646733</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>497</td>\n",
       "      <td>0.486526</td>\n",
       "      <td>0.501372</td>\n",
       "      <td>0.500175</td>\n",
       "      <td>0.508139</td>\n",
       "      <td>0.489411</td>\n",
       "      <td>0.457311</td>\n",
       "      <td>0.462321</td>\n",
       "      <td>0.462927</td>\n",
       "      <td>0.442647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787532</td>\n",
       "      <td>0.780347</td>\n",
       "      <td>0.768362</td>\n",
       "      <td>0.771411</td>\n",
       "      <td>0.770919</td>\n",
       "      <td>0.761464</td>\n",
       "      <td>0.770314</td>\n",
       "      <td>0.763324</td>\n",
       "      <td>0.797187</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>498</td>\n",
       "      <td>0.464595</td>\n",
       "      <td>0.498822</td>\n",
       "      <td>0.489077</td>\n",
       "      <td>0.453381</td>\n",
       "      <td>0.487636</td>\n",
       "      <td>0.461950</td>\n",
       "      <td>0.461671</td>\n",
       "      <td>0.447362</td>\n",
       "      <td>0.451952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739432</td>\n",
       "      <td>0.759722</td>\n",
       "      <td>0.752118</td>\n",
       "      <td>0.761910</td>\n",
       "      <td>0.761111</td>\n",
       "      <td>0.730431</td>\n",
       "      <td>0.753545</td>\n",
       "      <td>0.749619</td>\n",
       "      <td>0.756383</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>499</td>\n",
       "      <td>0.460840</td>\n",
       "      <td>0.457656</td>\n",
       "      <td>0.434632</td>\n",
       "      <td>0.412675</td>\n",
       "      <td>0.418638</td>\n",
       "      <td>0.408338</td>\n",
       "      <td>0.403807</td>\n",
       "      <td>0.388811</td>\n",
       "      <td>0.382484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717509</td>\n",
       "      <td>0.726149</td>\n",
       "      <td>0.728631</td>\n",
       "      <td>0.725808</td>\n",
       "      <td>0.716943</td>\n",
       "      <td>0.718320</td>\n",
       "      <td>0.707611</td>\n",
       "      <td>0.729484</td>\n",
       "      <td>0.718706</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468 rows Ã— 450 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index         0         1         2         3         4         5  \\\n",
       "0        0  0.416181  0.396844  0.408985  0.372865  0.385293  0.365390   \n",
       "1        1  0.415797  0.402956  0.402564  0.396014  0.397192  0.389634   \n",
       "2        2  0.389023  0.371206  0.373098  0.373872  0.361056  0.349709   \n",
       "3        3  0.468837  0.473255  0.462949  0.459335  0.461672  0.459824   \n",
       "4        4  0.483352  0.487274  0.469153  0.487648  0.464026  0.451152   \n",
       "..     ...       ...       ...       ...       ...       ...       ...   \n",
       "463    495  0.478140  0.444033  0.442120  0.437473  0.428672  0.413238   \n",
       "464    496  0.409367  0.394941  0.380236  0.375340  0.346122  0.354650   \n",
       "465    497  0.486526  0.501372  0.500175  0.508139  0.489411  0.457311   \n",
       "466    498  0.464595  0.498822  0.489077  0.453381  0.487636  0.461950   \n",
       "467    499  0.460840  0.457656  0.434632  0.412675  0.418638  0.408338   \n",
       "\n",
       "            6         7         8  ...       439       440       441  \\\n",
       "0    0.355226  0.343350  0.344837  ...  0.710280  0.717482  0.715078   \n",
       "1    0.375671  0.363689  0.373883  ...  0.684011  0.697271  0.701995   \n",
       "2    0.333882  0.330841  0.328925  ...  0.683054  0.669286  0.663179   \n",
       "3    0.458194  0.427737  0.415360  ...  0.742782  0.730801  0.736787   \n",
       "4    0.458229  0.440782  0.426193  ...  0.770227  0.773013  0.761431   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "463  0.417758  0.420388  0.413290  ...  0.747858  0.730535  0.716969   \n",
       "464  0.361170  0.342974  0.352137  ...  0.670232  0.659045  0.661587   \n",
       "465  0.462321  0.462927  0.442647  ...  0.787532  0.780347  0.768362   \n",
       "466  0.461671  0.447362  0.451952  ...  0.739432  0.759722  0.752118   \n",
       "467  0.403807  0.388811  0.382484  ...  0.717509  0.726149  0.728631   \n",
       "\n",
       "          442       443       444       445       446       447  vomitoxin_ppb  \n",
       "0    0.705379  0.696691  0.692793  0.711369  0.697679  0.704520         1100.0  \n",
       "1    0.696077  0.701012  0.677418  0.696921  0.696544  0.689054         1000.0  \n",
       "2    0.676165  0.676591  0.655951  0.658945  0.670989  0.665176         1300.0  \n",
       "3    0.730044  0.751437  0.738497  0.742446  0.754657  0.733474         1300.0  \n",
       "4    0.763488  0.762473  0.744012  0.775486  0.760431  0.751988          220.0  \n",
       "..        ...       ...       ...       ...       ...       ...            ...  \n",
       "463  0.739297  0.724827  0.720484  0.740626  0.740116  0.721839         1200.0  \n",
       "464  0.658422  0.644254  0.646479  0.656779  0.646700  0.646733            0.0  \n",
       "465  0.771411  0.770919  0.761464  0.770314  0.763324  0.797187            0.0  \n",
       "466  0.761910  0.761111  0.730431  0.753545  0.749619  0.756383            0.0  \n",
       "467  0.725808  0.716943  0.718320  0.707611  0.729484  0.718706         1400.0  \n",
       "\n",
       "[468 rows x 450 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kq/zdst5kxx4x39_9w4n35yjf_w0000gn/T/ipykernel_59544/1301887215.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned.drop('index',axis=1,inplace = True)\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.drop('index',axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_cleaned.drop('vomitoxin_ppb',axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df_cleaned['vomitoxin_ppb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(468, 448)\n",
      "Optimal number of PCA components: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(X_pca.shape)\n",
    "\n",
    "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1\n",
    "print(f\"Optimal number of PCA components: {n_components}\")\n",
    "\n",
    "# Apply PCA with optimal number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_pca = pd.DataFrame(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the transformed features again\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y = StandardScaler()\n",
    "Y_train = scaler_y.fit_transform(pd.DataFrame(Y_train))\n",
    "Y_test = scaler_y.transform(pd.DataFrame(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Improved ANN Model\n",
    "model = Sequential([\n",
    "    Dense(160, activation='relu', input_shape=(X_train.shape[1],)),  # HL1\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(128, activation='relu'),  # HL2\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(64, activation='relu'),  # HL3\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(1, activation='linear')  # Output Layer for Regression\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.losses.MeanSquaredError object at 0x31c0ba730>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "opt = tensorflow.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2.8522 - mae: 1.3038 - val_loss: 1.3373 - val_mae: 0.7417\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 2.3088 - mae: 1.1556 - val_loss: 1.2650 - val_mae: 0.7224\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 2.0074 - mae: 1.0695 - val_loss: 1.2779 - val_mae: 0.7073\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.9154 - mae: 1.0350 - val_loss: 1.2682 - val_mae: 0.7079\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.6265 - mae: 0.9632 - val_loss: 1.2360 - val_mae: 0.7075\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.7518 - mae: 1.0122 - val_loss: 1.1941 - val_mae: 0.6912\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.3987 - mae: 0.9096 - val_loss: 1.2183 - val_mae: 0.6682\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.5541 - mae: 0.8994 - val_loss: 1.2069 - val_mae: 0.6547\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2350 - mae: 0.8303 - val_loss: 1.1878 - val_mae: 0.6481\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.4205 - mae: 0.8787 - val_loss: 1.1679 - val_mae: 0.6640\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2257 - mae: 0.7985 - val_loss: 1.1500 - val_mae: 0.6523\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1398 - mae: 0.7926 - val_loss: 1.1373 - val_mae: 0.6354\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0468 - mae: 0.7528 - val_loss: 1.1461 - val_mae: 0.6318\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1582 - mae: 0.8065 - val_loss: 1.1799 - val_mae: 0.6413\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2270 - mae: 0.7969 - val_loss: 1.1419 - val_mae: 0.6465\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1164 - mae: 0.7991 - val_loss: 1.0947 - val_mae: 0.6318\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2146 - mae: 0.8202 - val_loss: 1.0824 - val_mae: 0.6197\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1456 - mae: 0.7676 - val_loss: 1.0890 - val_mae: 0.6195\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1052 - mae: 0.7590 - val_loss: 1.0891 - val_mae: 0.6165\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0904 - mae: 0.7872 - val_loss: 1.0847 - val_mae: 0.6067\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0012 - mae: 0.7388 - val_loss: 1.0712 - val_mae: 0.5994\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9417 - mae: 0.7164 - val_loss: 1.0684 - val_mae: 0.5928\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0401 - mae: 0.7370 - val_loss: 1.0416 - val_mae: 0.5878\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9417 - mae: 0.7118 - val_loss: 1.0170 - val_mae: 0.5948\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9162 - mae: 0.6998 - val_loss: 0.9984 - val_mae: 0.5936\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9522 - mae: 0.7281 - val_loss: 0.9708 - val_mae: 0.5778\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9013 - mae: 0.6972 - val_loss: 0.9606 - val_mae: 0.5708\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9245 - mae: 0.6881 - val_loss: 0.9539 - val_mae: 0.5696\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8928 - mae: 0.6792 - val_loss: 0.9022 - val_mae: 0.5636\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9420 - mae: 0.6917 - val_loss: 0.9006 - val_mae: 0.5736\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9674 - mae: 0.7256 - val_loss: 0.9136 - val_mae: 0.5685\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9181 - mae: 0.6972 - val_loss: 0.9429 - val_mae: 0.5735\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7695 - mae: 0.6411 - val_loss: 0.9296 - val_mae: 0.5736\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9594 - mae: 0.7044 - val_loss: 0.8858 - val_mae: 0.5658\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0093 - mae: 0.7263 - val_loss: 0.8781 - val_mae: 0.5709\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8167 - mae: 0.6403 - val_loss: 0.8807 - val_mae: 0.5752\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9257 - mae: 0.6666 - val_loss: 0.9053 - val_mae: 0.5830\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8916 - mae: 0.6723 - val_loss: 0.8974 - val_mae: 0.5819\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8302 - mae: 0.6635 - val_loss: 0.8721 - val_mae: 0.5756\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8155 - mae: 0.6461 - val_loss: 0.8839 - val_mae: 0.5792\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9036 - mae: 0.6812 - val_loss: 0.8734 - val_mae: 0.5724\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8002 - mae: 0.6356 - val_loss: 0.8572 - val_mae: 0.5639\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7904 - mae: 0.6275 - val_loss: 0.8468 - val_mae: 0.5581\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9543 - mae: 0.6874 - val_loss: 0.8492 - val_mae: 0.5552\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7654 - mae: 0.6180 - val_loss: 0.8599 - val_mae: 0.5578\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7898 - mae: 0.6193 - val_loss: 0.8616 - val_mae: 0.5586\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8384 - mae: 0.6422 - val_loss: 0.8507 - val_mae: 0.5576\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8073 - mae: 0.6391 - val_loss: 0.8373 - val_mae: 0.5577\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7637 - mae: 0.6245 - val_loss: 0.8407 - val_mae: 0.5592\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8185 - mae: 0.6522 - val_loss: 0.8448 - val_mae: 0.5630\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8139 - mae: 0.6437 - val_loss: 0.8586 - val_mae: 0.5673\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.8357 - mae: 0.6365 - val_loss: 0.8663 - val_mae: 0.5715\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7290 - mae: 0.6060 - val_loss: 0.8579 - val_mae: 0.5718\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8430 - mae: 0.6563 - val_loss: 0.8628 - val_mae: 0.5708\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8046 - mae: 0.6375 - val_loss: 0.8682 - val_mae: 0.5717\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7604 - mae: 0.6142 - val_loss: 0.8578 - val_mae: 0.5668\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7682 - mae: 0.6204 - val_loss: 0.8421 - val_mae: 0.5641\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7954 - mae: 0.6280 - val_loss: 0.8517 - val_mae: 0.5672\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7124 - mae: 0.5961 - val_loss: 0.8577 - val_mae: 0.5681\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8494 - mae: 0.6506 - val_loss: 0.8531 - val_mae: 0.5691\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7394 - mae: 0.6037 - val_loss: 0.8661 - val_mae: 0.5721\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7970 - mae: 0.6283 - val_loss: 0.8584 - val_mae: 0.5718\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7630 - mae: 0.6114 - val_loss: 0.8473 - val_mae: 0.5674\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7284 - mae: 0.6070 - val_loss: 0.8380 - val_mae: 0.5673\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7787 - mae: 0.6074 - val_loss: 0.8311 - val_mae: 0.5648\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7792 - mae: 0.6082 - val_loss: 0.8321 - val_mae: 0.5680\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8067 - mae: 0.6102 - val_loss: 0.8274 - val_mae: 0.5693\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8066 - mae: 0.6306 - val_loss: 0.8267 - val_mae: 0.5690\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7819 - mae: 0.6167 - val_loss: 0.8300 - val_mae: 0.5685\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7150 - mae: 0.6027 - val_loss: 0.8358 - val_mae: 0.5693\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7207 - mae: 0.6091 - val_loss: 0.8368 - val_mae: 0.5696\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7411 - mae: 0.5800 - val_loss: 0.8270 - val_mae: 0.5640\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6778 - mae: 0.5856 - val_loss: 0.8326 - val_mae: 0.5668\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7076 - mae: 0.6048 - val_loss: 0.8360 - val_mae: 0.5657\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7631 - mae: 0.6177 - val_loss: 0.8263 - val_mae: 0.5640\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7181 - mae: 0.5985 - val_loss: 0.8267 - val_mae: 0.5640\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7058 - mae: 0.5766 - val_loss: 0.8203 - val_mae: 0.5606\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7046 - mae: 0.5948 - val_loss: 0.8255 - val_mae: 0.5591\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7417 - mae: 0.5950 - val_loss: 0.8305 - val_mae: 0.5602\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6881 - mae: 0.5715 - val_loss: 0.8319 - val_mae: 0.5597\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7401 - mae: 0.5995 - val_loss: 0.8314 - val_mae: 0.5570\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7251 - mae: 0.5742 - val_loss: 0.8423 - val_mae: 0.5644\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7489 - mae: 0.5968 - val_loss: 0.8470 - val_mae: 0.5663\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7532 - mae: 0.6078 - val_loss: 0.8392 - val_mae: 0.5625\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6740 - mae: 0.5684 - val_loss: 0.8322 - val_mae: 0.5620\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7279 - mae: 0.5886 - val_loss: 0.8281 - val_mae: 0.5639\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6570 - mae: 0.5636 - val_loss: 0.8326 - val_mae: 0.5626\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7476 - mae: 0.6164 - val_loss: 0.8427 - val_mae: 0.5639\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7009 - mae: 0.5902 - val_loss: 0.8359 - val_mae: 0.5603\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6700 - mae: 0.5596 - val_loss: 0.8382 - val_mae: 0.5609\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6880 - mae: 0.5721 - val_loss: 0.8415 - val_mae: 0.5627\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6774 - mae: 0.5693 - val_loss: 0.8543 - val_mae: 0.5671\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7336 - mae: 0.5971 - val_loss: 0.8446 - val_mae: 0.5632\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6932 - mae: 0.5777 - val_loss: 0.8392 - val_mae: 0.5613\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6925 - mae: 0.5889 - val_loss: 0.8307 - val_mae: 0.5615\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7118 - mae: 0.5826 - val_loss: 0.8319 - val_mae: 0.5630\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - mae: 0.5782 - val_loss: 0.8332 - val_mae: 0.5631\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6700 - mae: 0.5782 - val_loss: 0.8311 - val_mae: 0.5618\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7183 - mae: 0.5857 - val_loss: 0.8245 - val_mae: 0.5563\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7342 - mae: 0.5911 - val_loss: 0.8382 - val_mae: 0.5621\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 160)               640       \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 160)               640       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 160)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 128)               20608     \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30977 (121.00 KB)\n",
      "Trainable params: 30273 (118.25 KB)\n",
      "Non-trainable params: 704 (2.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x3142bb820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x3142bb820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 731us/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8381962117253147 0.5621109554281531 0.3862726864545537\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "print(mse,mae,r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in ./venv/lib/python3.8/site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in ./venv/lib/python3.8/site-packages (from keras-tuner) (2.13.1)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.8/site-packages (from keras-tuner) (24.2)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.8/site-packages (from keras-tuner) (2.32.3)\n",
      "Requirement already satisfied: kt-legacy in ./venv/lib/python3.8/site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.8/site-packages (from requests->keras-tuner) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests->keras-tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests->keras-tuner) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests->keras-tuner) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from keras_tuner_results/ann_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 10ms/step - loss: 2.6289 - mae: 1.2678 - val_loss: 1.2206 - val_mae: 0.7607\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.8861 - mae: 1.0542 - val_loss: 1.2089 - val_mae: 0.7655\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.8255 - mae: 1.0156 - val_loss: 1.2344 - val_mae: 0.7808\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.5601 - mae: 0.9500 - val_loss: 1.2339 - val_mae: 0.7974\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.4400 - mae: 0.8977 - val_loss: 1.2199 - val_mae: 0.7859\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.4604 - mae: 0.9047 - val_loss: 1.1923 - val_mae: 0.7717\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2827 - mae: 0.8470 - val_loss: 1.1948 - val_mae: 0.7925\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2063 - mae: 0.8133 - val_loss: 1.1968 - val_mae: 0.8234\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1359 - mae: 0.7964 - val_loss: 1.1823 - val_mae: 0.7955\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2159 - mae: 0.8334 - val_loss: 1.1792 - val_mae: 0.7673\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1748 - mae: 0.8041 - val_loss: 1.1409 - val_mae: 0.7617\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1019 - mae: 0.7975 - val_loss: 1.1306 - val_mae: 0.7251\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1357 - mae: 0.7620 - val_loss: 1.1175 - val_mae: 0.7192\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1248 - mae: 0.7748 - val_loss: 1.1056 - val_mae: 0.7268\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0719 - mae: 0.7836 - val_loss: 1.0820 - val_mae: 0.7094\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1044 - mae: 0.7901 - val_loss: 1.0849 - val_mae: 0.7246\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0290 - mae: 0.7294 - val_loss: 1.0783 - val_mae: 0.7221\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0436 - mae: 0.7483 - val_loss: 1.0809 - val_mae: 0.7163\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1251 - mae: 0.7613 - val_loss: 1.0640 - val_mae: 0.6933\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9878 - mae: 0.7341 - val_loss: 1.0498 - val_mae: 0.6731\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9775 - mae: 0.7184 - val_loss: 1.0118 - val_mae: 0.7078\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0426 - mae: 0.7388 - val_loss: 1.0436 - val_mae: 0.7311\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9562 - mae: 0.7075 - val_loss: 1.0304 - val_mae: 0.7196\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9910 - mae: 0.7472 - val_loss: 0.9934 - val_mae: 0.6999\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9949 - mae: 0.7451 - val_loss: 0.9758 - val_mae: 0.6967\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9837 - mae: 0.7368 - val_loss: 0.9850 - val_mae: 0.6902\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0011 - mae: 0.7235 - val_loss: 0.9411 - val_mae: 0.6847\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8956 - mae: 0.6892 - val_loss: 0.9472 - val_mae: 0.6700\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8622 - mae: 0.6987 - val_loss: 0.9580 - val_mae: 0.6560\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0029 - mae: 0.7354 - val_loss: 0.9693 - val_mae: 0.6507\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0137 - mae: 0.7344 - val_loss: 0.9402 - val_mae: 0.6311\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9084 - mae: 0.7012 - val_loss: 0.9094 - val_mae: 0.6093\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8331 - mae: 0.6788 - val_loss: 0.8938 - val_mae: 0.6052\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8881 - mae: 0.6701 - val_loss: 0.8950 - val_mae: 0.6105\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8952 - mae: 0.6888 - val_loss: 0.9054 - val_mae: 0.6286\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9242 - mae: 0.7002 - val_loss: 0.9130 - val_mae: 0.6350\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9851 - mae: 0.7206 - val_loss: 0.8902 - val_mae: 0.6200\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9228 - mae: 0.7014 - val_loss: 0.8824 - val_mae: 0.6087\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8794 - mae: 0.6740 - val_loss: 0.8779 - val_mae: 0.5949\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9052 - mae: 0.6840 - val_loss: 0.8801 - val_mae: 0.5936\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8074 - mae: 0.6572 - val_loss: 0.8762 - val_mae: 0.5898\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9257 - mae: 0.6826 - val_loss: 0.8752 - val_mae: 0.5939\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7662 - mae: 0.6240 - val_loss: 0.8886 - val_mae: 0.6009\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7307 - mae: 0.6238 - val_loss: 0.8740 - val_mae: 0.5899\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8092 - mae: 0.6387 - val_loss: 0.8470 - val_mae: 0.5752\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8615 - mae: 0.6397 - val_loss: 0.8607 - val_mae: 0.5781\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8293 - mae: 0.6679 - val_loss: 0.9100 - val_mae: 0.6066\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8039 - mae: 0.6511 - val_loss: 0.9026 - val_mae: 0.6060\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8125 - mae: 0.6518 - val_loss: 0.8648 - val_mae: 0.5837\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8484 - mae: 0.6702 - val_loss: 0.8526 - val_mae: 0.5731\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8336 - mae: 0.6428 - val_loss: 0.8685 - val_mae: 0.5818\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7941 - mae: 0.6447 - val_loss: 0.8809 - val_mae: 0.5919\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7726 - mae: 0.6369 - val_loss: 0.8859 - val_mae: 0.5925\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7798 - mae: 0.6244 - val_loss: 0.8897 - val_mae: 0.5925\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8020 - mae: 0.6519 - val_loss: 0.8725 - val_mae: 0.5837\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7408 - mae: 0.6120 - val_loss: 0.8771 - val_mae: 0.5900\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7074 - mae: 0.5993 - val_loss: 0.8600 - val_mae: 0.5768\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7814 - mae: 0.6300 - val_loss: 0.8556 - val_mae: 0.5744\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8235 - mae: 0.6476 - val_loss: 0.8667 - val_mae: 0.5798\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7454 - mae: 0.6259 - val_loss: 0.8826 - val_mae: 0.5901\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7817 - mae: 0.6459 - val_loss: 0.8879 - val_mae: 0.5889\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8161 - mae: 0.6692 - val_loss: 0.8790 - val_mae: 0.5807\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7800 - mae: 0.6102 - val_loss: 0.8722 - val_mae: 0.5862\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7757 - mae: 0.6293 - val_loss: 0.8682 - val_mae: 0.5886\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8033 - mae: 0.6358 - val_loss: 0.8477 - val_mae: 0.5850\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7832 - mae: 0.6320 - val_loss: 0.8554 - val_mae: 0.5854\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7932 - mae: 0.6342 - val_loss: 0.8641 - val_mae: 0.5797\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7633 - mae: 0.6235 - val_loss: 0.8633 - val_mae: 0.5800\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7541 - mae: 0.6056 - val_loss: 0.8583 - val_mae: 0.5809\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.7575 - mae: 0.6131 - val_loss: 0.8709 - val_mae: 0.5872\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7837 - mae: 0.6282 - val_loss: 0.8620 - val_mae: 0.5880\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7215 - mae: 0.5965 - val_loss: 0.8625 - val_mae: 0.5890\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7106 - mae: 0.5983 - val_loss: 0.8608 - val_mae: 0.5830\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7463 - mae: 0.6103 - val_loss: 0.8429 - val_mae: 0.5756\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7680 - mae: 0.6155 - val_loss: 0.8414 - val_mae: 0.5774\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7063 - mae: 0.6020 - val_loss: 0.8326 - val_mae: 0.5746\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7274 - mae: 0.6078 - val_loss: 0.8353 - val_mae: 0.5694\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7285 - mae: 0.6008 - val_loss: 0.8525 - val_mae: 0.5787\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7144 - mae: 0.6015 - val_loss: 0.8481 - val_mae: 0.5787\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6916 - mae: 0.5874 - val_loss: 0.8317 - val_mae: 0.5657\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7349 - mae: 0.6116 - val_loss: 0.8494 - val_mae: 0.5763\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7558 - mae: 0.6046 - val_loss: 0.8672 - val_mae: 0.5823\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6733 - mae: 0.5763 - val_loss: 0.8713 - val_mae: 0.5871\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6659 - mae: 0.5722 - val_loss: 0.8610 - val_mae: 0.5787\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7007 - mae: 0.5747 - val_loss: 0.8623 - val_mae: 0.5727\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7077 - mae: 0.5993 - val_loss: 0.8435 - val_mae: 0.5686\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7409 - mae: 0.5977 - val_loss: 0.8633 - val_mae: 0.5788\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7181 - mae: 0.5900 - val_loss: 0.8572 - val_mae: 0.5729\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7791 - mae: 0.6078 - val_loss: 0.8577 - val_mae: 0.5760\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7252 - mae: 0.5870 - val_loss: 0.8562 - val_mae: 0.5806\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7487 - mae: 0.5960 - val_loss: 0.8512 - val_mae: 0.5750\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6892 - mae: 0.5835 - val_loss: 0.8642 - val_mae: 0.5844\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6897 - mae: 0.5917 - val_loss: 0.8586 - val_mae: 0.5810\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7167 - mae: 0.5865 - val_loss: 0.8503 - val_mae: 0.5723\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7094 - mae: 0.5699 - val_loss: 0.8436 - val_mae: 0.5682\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6774 - mae: 0.5696 - val_loss: 0.8451 - val_mae: 0.5702\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6970 - mae: 0.5749 - val_loss: 0.8479 - val_mae: 0.5724\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7165 - mae: 0.5906 - val_loss: 0.8628 - val_mae: 0.5853\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7689 - mae: 0.6084 - val_loss: 0.8633 - val_mae: 0.5854\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6846 - mae: 0.5817 - val_loss: 0.8460 - val_mae: 0.5777\n",
      "3/3 [==============================] - 0s 700us/step\n",
      "Best Hyperparameters:\n",
      "units_1: 160\n",
      "activation_1: relu\n",
      "dropout_1: 0.4\n",
      "units_2: 128\n",
      "activation_2: relu\n",
      "dropout_2: 0.4\n",
      "units_3: 64\n",
      "activation_3: elu\n",
      "dropout_3: 0.2\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 100\n",
      "tuner/initial_epoch: 34\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0230\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Function to build the ANN model\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input Layer + First Hidden Layer\n",
    "    model.add(Dense(\n",
    "        hp.Int('units_1', min_value=32, max_value=256, step=32),\n",
    "        activation=hp.Choice('activation_1', values=['relu', 'elu']),\n",
    "        input_shape=(X_train.shape[1],)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_1', 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    model.add(Dense(\n",
    "        hp.Int('units_2', min_value=32, max_value=128, step=32),\n",
    "        activation=hp.Choice('activation_2', values=['relu', 'elu'])\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_2', 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # Third Hidden Layer\n",
    "    model.add(Dense(\n",
    "        hp.Int('units_3', min_value=32, max_value=128, step=32),\n",
    "        activation=hp.Choice('activation_3', values=['relu', 'elu'])\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_3', 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[0.001, 0.0005, 0.0001])\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize Keras Tuner (Hyperband Search)\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=100,\n",
    "    factor=3,\n",
    "    directory='keras_tuner_results',\n",
    "    project_name='ann_tuning'\n",
    ")\n",
    "\n",
    "# Search for best hyperparameters\n",
    "tuner.search(X_train, Y_train, epochs=50, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Get the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model\n",
    "history = best_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "Y_pred = best_model.predict(X_test)\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key in best_hps.values.keys():\n",
    "    print(f\"{key}: {best_hps.get(key)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "units_1: 160\n",
      "activation_1: relu\n",
      "dropout_1: 0.4\n",
      "units_2: 128\n",
      "activation_2: relu\n",
      "dropout_2: 0.4\n",
      "units_3: 64\n",
      "activation_3: elu\n",
      "dropout_3: 0.2\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 100\n",
      "tuner/initial_epoch: 34\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0230\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparameters:\")\n",
    "for key in best_hps.values.keys():\n",
    "    print(f\"{key}: {best_hps.get(key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2.2960 - mae: 1.1527 - val_loss: 1.2685 - val_mae: 0.7686\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.9768 - mae: 1.0719 - val_loss: 1.2919 - val_mae: 0.7679\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.6963 - mae: 0.9815 - val_loss: 1.2797 - val_mae: 0.7713\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.6133 - mae: 0.9303 - val_loss: 1.2702 - val_mae: 0.7689\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.4756 - mae: 0.9217 - val_loss: 1.2544 - val_mae: 0.7643\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2462 - mae: 0.8501 - val_loss: 1.2567 - val_mae: 0.7487\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.3265 - mae: 0.8791 - val_loss: 1.2408 - val_mae: 0.7534\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1966 - mae: 0.8314 - val_loss: 1.2345 - val_mae: 0.7836\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2251 - mae: 0.8317 - val_loss: 1.2491 - val_mae: 0.7973\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2483 - mae: 0.8415 - val_loss: 1.2438 - val_mae: 0.8102\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0736 - mae: 0.7854 - val_loss: 1.2316 - val_mae: 0.7847\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2065 - mae: 0.8178 - val_loss: 1.2022 - val_mae: 0.7996\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0160 - mae: 0.7664 - val_loss: 1.1824 - val_mae: 0.7681\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2139 - mae: 0.8154 - val_loss: 1.1851 - val_mae: 0.7942\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0465 - mae: 0.7641 - val_loss: 1.1653 - val_mae: 0.7747\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1085 - mae: 0.7841 - val_loss: 1.1729 - val_mae: 0.8058\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2138 - mae: 0.8285 - val_loss: 1.1801 - val_mae: 0.8405\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9906 - mae: 0.7370 - val_loss: 1.1650 - val_mae: 0.8337\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0555 - mae: 0.7383 - val_loss: 1.1569 - val_mae: 0.8417\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0572 - mae: 0.7759 - val_loss: 1.1440 - val_mae: 0.8324\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0149 - mae: 0.7606 - val_loss: 1.1331 - val_mae: 0.7985\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9226 - mae: 0.6991 - val_loss: 1.0865 - val_mae: 0.7567\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0775 - mae: 0.7676 - val_loss: 1.0476 - val_mae: 0.7017\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9396 - mae: 0.7094 - val_loss: 1.0666 - val_mae: 0.7175\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0565 - mae: 0.7373 - val_loss: 1.0735 - val_mae: 0.7563\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9796 - mae: 0.7326 - val_loss: 1.0243 - val_mae: 0.7215\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9555 - mae: 0.7027 - val_loss: 1.0363 - val_mae: 0.7187\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9857 - mae: 0.6953 - val_loss: 1.0378 - val_mae: 0.7210\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9786 - mae: 0.7272 - val_loss: 1.0079 - val_mae: 0.7157\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9552 - mae: 0.7280 - val_loss: 0.9823 - val_mae: 0.6927\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9016 - mae: 0.6969 - val_loss: 0.9718 - val_mae: 0.6636\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9270 - mae: 0.6927 - val_loss: 0.9467 - val_mae: 0.6554\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9145 - mae: 0.7022 - val_loss: 0.9429 - val_mae: 0.6619\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8993 - mae: 0.7008 - val_loss: 0.9243 - val_mae: 0.6515\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8972 - mae: 0.6871 - val_loss: 0.9174 - val_mae: 0.6445\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9754 - mae: 0.7144 - val_loss: 0.9120 - val_mae: 0.6444\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8885 - mae: 0.6747 - val_loss: 0.9047 - val_mae: 0.6394\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8874 - mae: 0.6881 - val_loss: 0.9039 - val_mae: 0.6315\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8640 - mae: 0.6671 - val_loss: 0.9006 - val_mae: 0.6296\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8038 - mae: 0.6565 - val_loss: 0.8883 - val_mae: 0.6216\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8994 - mae: 0.6825 - val_loss: 0.8645 - val_mae: 0.6157\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7665 - mae: 0.6390 - val_loss: 0.8661 - val_mae: 0.6076\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.7842 - mae: 0.6451 - val_loss: 0.8723 - val_mae: 0.5943\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7793 - mae: 0.6324 - val_loss: 0.8494 - val_mae: 0.5892\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8645 - mae: 0.6602 - val_loss: 0.8541 - val_mae: 0.5908\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8959 - mae: 0.6766 - val_loss: 0.8572 - val_mae: 0.5845\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8107 - mae: 0.6478 - val_loss: 0.8543 - val_mae: 0.5846\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7578 - mae: 0.6235 - val_loss: 0.8533 - val_mae: 0.5864\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8401 - mae: 0.6548 - val_loss: 0.8504 - val_mae: 0.5864\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8576 - mae: 0.6666 - val_loss: 0.8604 - val_mae: 0.5883\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7681 - mae: 0.6308 - val_loss: 0.8440 - val_mae: 0.5764\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7901 - mae: 0.6499 - val_loss: 0.8259 - val_mae: 0.5697\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7821 - mae: 0.6224 - val_loss: 0.8184 - val_mae: 0.5688\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8285 - mae: 0.6585 - val_loss: 0.8299 - val_mae: 0.5704\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7217 - mae: 0.6069 - val_loss: 0.8306 - val_mae: 0.5639\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7766 - mae: 0.6110 - val_loss: 0.8355 - val_mae: 0.5651\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8464 - mae: 0.6599 - val_loss: 0.8438 - val_mae: 0.5755\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7430 - mae: 0.6168 - val_loss: 0.8423 - val_mae: 0.5780\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7985 - mae: 0.6330 - val_loss: 0.8369 - val_mae: 0.5743\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8391 - mae: 0.6502 - val_loss: 0.8265 - val_mae: 0.5688\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7678 - mae: 0.6311 - val_loss: 0.8271 - val_mae: 0.5694\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7036 - mae: 0.5967 - val_loss: 0.8331 - val_mae: 0.5642\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7413 - mae: 0.6139 - val_loss: 0.8519 - val_mae: 0.5727\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7362 - mae: 0.6019 - val_loss: 0.8526 - val_mae: 0.5746\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7183 - mae: 0.6044 - val_loss: 0.8329 - val_mae: 0.5692\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7295 - mae: 0.6046 - val_loss: 0.8361 - val_mae: 0.5729\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7949 - mae: 0.6281 - val_loss: 0.8364 - val_mae: 0.5718\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7956 - mae: 0.6303 - val_loss: 0.8469 - val_mae: 0.5769\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7694 - mae: 0.6144 - val_loss: 0.8368 - val_mae: 0.5680\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7968 - mae: 0.6318 - val_loss: 0.8400 - val_mae: 0.5640\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7453 - mae: 0.6197 - val_loss: 0.8338 - val_mae: 0.5683\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8272 - mae: 0.6273 - val_loss: 0.8314 - val_mae: 0.5682\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7658 - mae: 0.6206 - val_loss: 0.8450 - val_mae: 0.5742\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7212 - mae: 0.5894 - val_loss: 0.8500 - val_mae: 0.5795\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7024 - mae: 0.5956 - val_loss: 0.8365 - val_mae: 0.5742\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7062 - mae: 0.6025 - val_loss: 0.8338 - val_mae: 0.5715\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7143 - mae: 0.6108 - val_loss: 0.8438 - val_mae: 0.5696\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6918 - mae: 0.5699 - val_loss: 0.8385 - val_mae: 0.5666\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7380 - mae: 0.6103 - val_loss: 0.8213 - val_mae: 0.5626\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6820 - mae: 0.5731 - val_loss: 0.8334 - val_mae: 0.5656\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6560 - mae: 0.5744 - val_loss: 0.8288 - val_mae: 0.5648\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7226 - mae: 0.5991 - val_loss: 0.8509 - val_mae: 0.5671\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7286 - mae: 0.5867 - val_loss: 0.8340 - val_mae: 0.5630\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7552 - mae: 0.6064 - val_loss: 0.8332 - val_mae: 0.5642\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6953 - mae: 0.5923 - val_loss: 0.8439 - val_mae: 0.5711\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7235 - mae: 0.5884 - val_loss: 0.8345 - val_mae: 0.5658\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7149 - mae: 0.5894 - val_loss: 0.8303 - val_mae: 0.5672\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7490 - mae: 0.5989 - val_loss: 0.8454 - val_mae: 0.5724\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6871 - mae: 0.5920 - val_loss: 0.8525 - val_mae: 0.5754\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6778 - mae: 0.5784 - val_loss: 0.8652 - val_mae: 0.5721\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7681 - mae: 0.6257 - val_loss: 0.8572 - val_mae: 0.5738\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6276 - mae: 0.5479 - val_loss: 0.8488 - val_mae: 0.5735\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6839 - mae: 0.5619 - val_loss: 0.8566 - val_mae: 0.5771\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7030 - mae: 0.5837 - val_loss: 0.8643 - val_mae: 0.5778\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7150 - mae: 0.5891 - val_loss: 0.8574 - val_mae: 0.5762\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6337 - mae: 0.5668 - val_loss: 0.8628 - val_mae: 0.5721\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7324 - mae: 0.6077 - val_loss: 0.8522 - val_mae: 0.5694\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7084 - mae: 0.5902 - val_loss: 0.8656 - val_mae: 0.5719\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7013 - mae: 0.5803 - val_loss: 0.8629 - val_mae: 0.5749\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6842 - mae: 0.5793 - val_loss: 0.8472 - val_mae: 0.5679\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 160)               640       \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 160)               640       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 160)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 128)               20608     \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30977 (121.00 KB)\n",
      "Trainable params: 30273 (118.25 KB)\n",
      "Non-trainable params: 704 (2.75 KB)\n",
      "_________________________________________________________________\n",
      "3/3 [==============================] - 0s 747us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build the model with hyperparameter-tuned values\n",
    "model = Sequential([\n",
    "    Dense(160, activation='relu', input_shape=(X_train.shape[1],)),  # HL1 - Updated to 160 units\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),  # Updated dropout\n",
    "\n",
    "    Dense(128, activation='relu'),  # HL2 - Updated to 128 units\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),  # Updated dropout\n",
    "\n",
    "    Dense(64, activation='elu'),  # HL3 - Updated to 64 units, changed activation to ELU\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),  # Updated dropout\n",
    "\n",
    "    Dense(1, activation='linear')  # Output layer (Regression)\n",
    "])\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train Model using tuned epochs\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    epochs=100,  # Updated to 100 as per tuning\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Make Predictions\n",
    "Y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8471505154326717 0.5679262395639423 0.37971634477451366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "print(mse,mae,r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hardikpandey/Desktop/wd/V.2 ds/imago project/venv/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('tuned_ann_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
